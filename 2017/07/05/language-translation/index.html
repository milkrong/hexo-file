<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="Language TranslationIn this project, you’re going to take a peek into the realm of neural network machine translation.  You’ll be training a sequence to sequence model on a dataset of English and French sentences that can translate new sentences from English to French.
Get the DataSince translating the whole language of English to French will take lots of time to train, we have provided you with a small portion of the English corpus.
12345678910&quot;&quot;&quot;DON&#39;T MODIFY ANYTHING IN THIS CELL&quot;&quot;&quot;import helperimport problem_unittests as testssource_path = &#39;data/small_vocab_en&#39;target_path = &#39;data/small_vocab_fr&#39;source_text = helper.load_data(source_path)target_text = helper.load_data(target_path)
Explore the DataPlay around with view_sentence_range to view different parts of the data.">
    

    <!--Author-->
    
        <meta name="author" content="Eric Liu">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="language-translation"/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="MilkSite"/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

    <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>language-translation - MilkSite</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Google Analytics -->
    


</head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about.html">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
    </div>
</header>

        <section class="main">
            
<div class="post">

    <div class="post-header">
        <h1 class="title">
            <a href="/2017/07/05/language-translation/">
                language-translation
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2017-07-05</span>
            
            
            
        </div>
    </div>

    <div class="content">

        <!-- Gallery -->
        

        <!-- Post Content -->
        <h1 id="Language-Translation"><a href="#Language-Translation" class="headerlink" title="Language Translation"></a>Language Translation</h1><p>In this project, you’re going to take a peek into the realm of neural network machine translation.  You’ll be training a sequence to sequence model on a dataset of English and French sentences that can translate new sentences from English to French.</p>
<h2 id="Get-the-Data"><a href="#Get-the-Data" class="headerlink" title="Get the Data"></a>Get the Data</h2><p>Since translating the whole language of English to French will take lots of time to train, we have provided you with a small portion of the English corpus.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""</span></div><div class="line">DON'T MODIFY ANYTHING IN THIS CELL</div><div class="line">"""</div><div class="line"><span class="keyword">import</span> helper</div><div class="line"><span class="keyword">import</span> problem_unittests <span class="keyword">as</span> tests</div><div class="line"></div><div class="line">source_path = <span class="string">'data/small_vocab_en'</span></div><div class="line">target_path = <span class="string">'data/small_vocab_fr'</span></div><div class="line">source_text = helper.load_data(source_path)</div><div class="line">target_text = helper.load_data(target_path)</div></pre></td></tr></table></figure>
<h2 id="Explore-the-Data"><a href="#Explore-the-Data" class="headerlink" title="Explore the Data"></a>Explore the Data</h2><p>Play around with view_sentence_range to view different parts of the data.</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">view_sentence_range = (<span class="number">0</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line">DON'T MODIFY ANYTHING IN THIS CELL</div><div class="line">"""</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">print(<span class="string">'Dataset Stats'</span>)</div><div class="line">print(<span class="string">'Roughly the number of unique words: &#123;&#125;'</span>.format(len(&#123;word: <span class="keyword">None</span> <span class="keyword">for</span> word <span class="keyword">in</span> source_text.split()&#125;)))</div><div class="line"></div><div class="line">sentences = source_text.split(<span class="string">'\n'</span>)</div><div class="line">word_counts = [len(sentence.split()) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</div><div class="line">print(<span class="string">'Number of sentences: &#123;&#125;'</span>.format(len(sentences)))</div><div class="line">print(<span class="string">'Average number of words in a sentence: &#123;&#125;'</span>.format(np.average(word_counts)))</div><div class="line"></div><div class="line">print()</div><div class="line">print(<span class="string">'English sentences &#123;&#125; to &#123;&#125;:'</span>.format(*view_sentence_range))</div><div class="line">print(<span class="string">'\n'</span>.join(source_text.split(<span class="string">'\n'</span>)[view_sentence_range[<span class="number">0</span>]:view_sentence_range[<span class="number">1</span>]]))</div><div class="line">print()</div><div class="line">print(<span class="string">'French sentences &#123;&#125; to &#123;&#125;:'</span>.format(*view_sentence_range))</div><div class="line">print(<span class="string">'\n'</span>.join(target_text.split(<span class="string">'\n'</span>)[view_sentence_range[<span class="number">0</span>]:view_sentence_range[<span class="number">1</span>]]))</div></pre></td></tr></table></figure>
<h2 id="Implement-Preprocessing-Function"><a href="#Implement-Preprocessing-Function" class="headerlink" title="Implement Preprocessing Function"></a>Implement Preprocessing Function</h2><h3 id="Text-to-Word-Ids"><a href="#Text-to-Word-Ids" class="headerlink" title="Text to Word Ids"></a>Text to Word Ids</h3><p>As you did with other RNNs, you must turn the text into a number so the computer can understand it. In the function <code>text_to_ids()</code>, you’ll turn <code>source_text</code> and <code>target_text</code> from words to ids.  However, you need to add the <code>&lt;EOS&gt;</code> word id at the end of <code>target_text</code>.  This will help the neural network predict when the sentence should end.</p>
<p>You can get the <code>&lt;EOS&gt;</code> word id by doing:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">target_vocab_to_int[<span class="string">'&lt;EOS&gt;'</span>]</div></pre></td></tr></table></figure></p>
<p>You can get other word ids using <code>source_vocab_to_int</code> and <code>target_vocab_to_int</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">text_to_ids</span><span class="params">(source_text, target_text, source_vocab_to_int, target_vocab_to_int)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Convert source and target text to proper word ids</div><div class="line">    :param source_text: String that contains all the source text.</div><div class="line">    :param target_text: String that contains all the target text.</div><div class="line">    :param source_vocab_to_int: Dictionary to go from the source words to an id</div><div class="line">    :param target_vocab_to_int: Dictionary to go from the target words to an id</div><div class="line">    :return: A tuple of lists (source_id_text, target_id_text)</div><div class="line">    """</div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement Function</span></div><div class="line">    source_id_text = [[source_vocab_to_int[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentence.split()] </div><div class="line">                      <span class="keyword">for</span> sentence <span class="keyword">in</span> source_text.split(<span class="string">'\n'</span>)]</div><div class="line">    target_id_text = [[target_vocab_to_int[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentence.split()] + [target_vocab_to_int[<span class="string">'&lt;EOS&gt;'</span>]] </div><div class="line">                      <span class="keyword">for</span> sentence <span class="keyword">in</span> target_text.split(<span class="string">'\n'</span>)]</div><div class="line">    <span class="keyword">return</span> source_id_text, target_id_text</div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line">DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</div><div class="line">"""</div><div class="line">tests.test_text_to_ids(text_to_ids)</div></pre></td></tr></table></figure>
<h3 id="Preprocess-all-the-data-and-save-it"><a href="#Preprocess-all-the-data-and-save-it" class="headerlink" title="Preprocess all the data and save it"></a>Preprocess all the data and save it</h3><p>Running the code cell below will preprocess all the data and save it to file.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""</span></div><div class="line">DON'T MODIFY ANYTHING IN THIS CELL</div><div class="line">"""</div><div class="line">helper.preprocess_and_save_data(source_path, target_path, text_to_ids)</div></pre></td></tr></table></figure>
<h1 id="Check-Point"><a href="#Check-Point" class="headerlink" title="Check Point"></a>Check Point</h1><p>This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""</span></div><div class="line">DON'T MODIFY ANYTHING IN THIS CELL</div><div class="line">"""</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> helper</div><div class="line"></div><div class="line">(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper.load_preprocess()</div></pre></td></tr></table></figure>
<h3 id="Check-the-Version-of-TensorFlow-and-Access-to-GPU"><a href="#Check-the-Version-of-TensorFlow-and-Access-to-GPU" class="headerlink" title="Check the Version of TensorFlow and Access to GPU"></a>Check the Version of TensorFlow and Access to GPU</h3><p>This will check to make sure you have the correct version of TensorFlow and access to a GPU</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""</span></div><div class="line">DON'T MODIFY ANYTHING IN THIS CELL</div><div class="line">"""</div><div class="line"><span class="keyword">from</span> distutils.version <span class="keyword">import</span> LooseVersion</div><div class="line"><span class="keyword">import</span> warnings</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">from</span> tensorflow.python.layers.core <span class="keyword">import</span> Dense</div><div class="line"></div><div class="line"><span class="comment"># Check TensorFlow Version</span></div><div class="line"><span class="keyword">assert</span> LooseVersion(tf.__version__) &gt;= LooseVersion(<span class="string">'1.1'</span>), <span class="string">'Please use TensorFlow version 1.1 or newer'</span></div><div class="line">print(<span class="string">'TensorFlow Version: &#123;&#125;'</span>.format(tf.__version__))</div><div class="line"></div><div class="line"><span class="comment"># Check for a GPU</span></div><div class="line"><span class="keyword">if</span> <span class="keyword">not</span> tf.test.gpu_device_name():</div><div class="line">    warnings.warn(<span class="string">'No GPU found. Please use a GPU to train your neural network.'</span>)</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    print(<span class="string">'Default GPU Device: &#123;&#125;'</span>.format(tf.test.gpu_device_name()))</div></pre></td></tr></table></figure>
<h2 id="Build-the-Neural-Network"><a href="#Build-the-Neural-Network" class="headerlink" title="Build the Neural Network"></a>Build the Neural Network</h2><p>You’ll build the components necessary to build a Sequence-to-Sequence model by implementing the following functions below:</p>
<ul>
<li><code>model_inputs</code></li>
<li><code>process_decoder_input</code></li>
<li><code>encoding_layer</code></li>
<li><code>decoding_layer_train</code></li>
<li><code>decoding_layer_infer</code></li>
<li><code>decoding_layer</code></li>
<li><code>seq2seq_model</code></li>
</ul>
<h3 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h3><p>Implement the <code>model_inputs()</code> function to create TF Placeholders for the Neural Network. It should create the following placeholders:</p>
<ul>
<li>Input text placeholder named “input” using the TF Placeholder name parameter with rank 2.</li>
<li>Targets placeholder with rank 2.</li>
<li>Learning rate placeholder with rank 0.</li>
<li>Keep probability placeholder named “keep_prob” using the TF Placeholder name parameter with rank 0.</li>
<li>Target sequence length placeholder named “target_sequence_length” with rank 1</li>
<li>Max target sequence length tensor named “max_target_len” getting its value from applying tf.reduce_max on the target_sequence_length placeholder. Rank 0.</li>
<li>Source sequence length placeholder named “source_sequence_length” with rank 1</li>
</ul>
<p>Return the placeholders in the following the tuple (input, targets, learning rate, keep probability, target sequence length, max target sequence length, source sequence length)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_inputs</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Create TF Placeholders for input, targets, learning rate, and lengths of source and target sequences.</div><div class="line">    :return: Tuple (input, targets, learning rate, keep probability, target sequence length,</div><div class="line">    max target sequence length, source sequence length)</div><div class="line">    """</div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement Function</span></div><div class="line">    inputs = tf.placeholder(tf.int32, [<span class="keyword">None</span>, <span class="keyword">None</span>], name=<span class="string">"input"</span>)</div><div class="line">    targets = tf.placeholder(tf.int32, [<span class="keyword">None</span>, <span class="keyword">None</span>], name=<span class="string">"targets"</span>)</div><div class="line">    learning_rate = tf.placeholder(tf.float32, name=<span class="string">"learning_rate"</span>)</div><div class="line">    keep_prob = tf.placeholder(tf.float32, name=<span class="string">"keep_prob"</span>)</div><div class="line">    target_seq_len = tf.placeholder(tf.int32, (<span class="keyword">None</span>,), name=<span class="string">"target_sequence_length"</span>)</div><div class="line">    max_target_len = tf.reduce_max(target_seq_len)</div><div class="line">    source_seq_len = tf.placeholder(tf.int32, (<span class="keyword">None</span>,), name=<span class="string">"source_sequence_length"</span>)</div><div class="line">    <span class="keyword">return</span> inputs, targets, learning_rate, keep_prob, target_seq_len, max_target_len, source_seq_len</div><div class="line"></div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line">DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</div><div class="line">"""</div><div class="line">tests.test_model_inputs(model_inputs)</div></pre></td></tr></table></figure>
<h3 id="Process-Decoder-Input"><a href="#Process-Decoder-Input" class="headerlink" title="Process Decoder Input"></a>Process Decoder Input</h3><p>Implement <code>process_decoder_input</code> by removing the last word id from each batch in <code>target_data</code> and concat the GO ID to the begining of each batch.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_decoder_input</span><span class="params">(target_data, target_vocab_to_int, batch_size)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Preprocess target data for encoding</div><div class="line">    :param target_data: Target Placehoder</div><div class="line">    :param target_vocab_to_int: Dictionary to go from the target words to an id</div><div class="line">    :param batch_size: Batch Size</div><div class="line">    :return: Preprocessed target data</div><div class="line">    """</div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement Function</span></div><div class="line">    ending = tf.strided_slice(target_data, [<span class="number">0</span>, <span class="number">0</span>], [batch_size, <span class="number">-1</span>], [<span class="number">1</span>, <span class="number">1</span>])</div><div class="line">    dec_input = tf.concat([tf.fill([batch_size, <span class="number">1</span>], target_vocab_to_int[<span class="string">'&lt;GO&gt;'</span>]), ending], <span class="number">1</span>)</div><div class="line">    <span class="keyword">return</span> dec_input</div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line">DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</div><div class="line">"""</div><div class="line">tests.test_process_encoding_input(process_decoder_input)</div></pre></td></tr></table></figure>
<h3 id="Encoding"><a href="#Encoding" class="headerlink" title="Encoding"></a>Encoding</h3><p>Implement <code>encoding_layer()</code> to create a Encoder RNN layer:</p>
<ul>
<li>Embed the encoder input using <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embed_sequence" target="_blank" rel="external"><code>tf.contrib.layers.embed_sequence</code></a></li>
<li>Construct a <a href="https://github.com/tensorflow/tensorflow/blob/6947f65a374ebf29e74bb71e36fd82760056d82c/tensorflow/docs_src/tutorials/recurrent.md#stacking-multiple-lstms" target="_blank" rel="external">stacked</a> <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell" target="_blank" rel="external"><code>tf.contrib.rnn.LSTMCell</code></a> wrapped in a <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper" target="_blank" rel="external"><code>tf.contrib.rnn.DropoutWrapper</code></a></li>
<li>Pass cell and embedded input to <a href="https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn" target="_blank" rel="external"><code>tf.nn.dynamic_rnn()</code></a></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> imp <span class="keyword">import</span> reload</div><div class="line">reload(tests)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">encoding_layer</span><span class="params">(rnn_inputs, rnn_size, num_layers, keep_prob, </span></span></div><div class="line">                   source_sequence_length, source_vocab_size, </div><div class="line">                   encoding_embedding_size):</div><div class="line">    <span class="string">"""</span></div><div class="line">    Create encoding layer</div><div class="line">    :param rnn_inputs: Inputs for the RNN</div><div class="line">    :param rnn_size: RNN Size</div><div class="line">    :param num_layers: Number of layers</div><div class="line">    :param keep_prob: Dropout keep probability</div><div class="line">    :param source_sequence_length: a list of the lengths of each sequence in the batch</div><div class="line">    :param source_vocab_size: vocabulary size of source data</div><div class="line">    :param encoding_embedding_size: embedding size of source data</div><div class="line">    :return: tuple (RNN output, RNN state)</div><div class="line">    """</div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement Function</span></div><div class="line">    encode_embed = tf.contrib.layers.embed_sequence(rnn_inputs, source_vocab_size,</div><div class="line">                                                    encoding_embedding_size)</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_cell</span><span class="params">(rnn_size)</span>:</span></div><div class="line">        enc_cell = tf.contrib.rnn.LSTMCell(rnn_size,</div><div class="line">                                          initializer=tf.random_uniform_initializer(<span class="number">-0.1</span>, <span class="number">0.1</span>, seed=<span class="number">2</span>))</div><div class="line">        </div><div class="line">        drop = tf.contrib.rnn.DropoutWrapper(enc_cell, output_keep_prob=keep_prob)</div><div class="line">        </div><div class="line">        <span class="keyword">return</span> drop</div><div class="line">    </div><div class="line">    enc_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers)])</div><div class="line">    </div><div class="line">    enc_output, enc_state = tf.nn.dynamic_rnn(enc_cell, encode_embed, </div><div class="line">                                              sequence_length=source_sequence_length,</div><div class="line">                                             dtype=tf.float32)</div><div class="line">    <span class="keyword">return</span> enc_output, enc_state</div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line">DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</div><div class="line">"""</div><div class="line">tests.test_encoding_layer(encoding_layer)</div></pre></td></tr></table></figure>
<h3 id="Decoding-Training"><a href="#Decoding-Training" class="headerlink" title="Decoding - Training"></a>Decoding - Training</h3><p>Create a training decoding layer:</p>
<ul>
<li>Create a <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/TrainingHelper" target="_blank" rel="external"><code>tf.contrib.seq2seq.TrainingHelper</code></a> </li>
<li>Create a <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BasicDecoder" target="_blank" rel="external"><code>tf.contrib.seq2seq.BasicDecoder</code></a></li>
<li>Obtain the decoder outputs from <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode" target="_blank" rel="external"><code>tf.contrib.seq2seq.dynamic_decode</code></a></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoding_layer_train</span><span class="params">(encoder_state, dec_cell, dec_embed_input, </span></span></div><div class="line">                         target_sequence_length, max_summary_length, </div><div class="line">                         output_layer, keep_prob):</div><div class="line">    <span class="string">"""</span></div><div class="line">    Create a decoding layer for training</div><div class="line">    :param encoder_state: Encoder State</div><div class="line">    :param dec_cell: Decoder RNN Cell</div><div class="line">    :param dec_embed_input: Decoder embedded input</div><div class="line">    :param target_sequence_length: The lengths of each sequence in the target batch</div><div class="line">    :param max_summary_length: The length of the longest sequence in the batch</div><div class="line">    :param output_layer: Function to apply the output layer</div><div class="line">    :param keep_prob: Dropout keep probability</div><div class="line">    :return: BasicDecoderOutput containing training logits and sample_id</div><div class="line">    """</div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement Function</span></div><div class="line">    </div><div class="line">    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,</div><div class="line">                                                       sequence_length=target_sequence_length,</div><div class="line">                                                       time_major=<span class="keyword">False</span>)</div><div class="line">    </div><div class="line">    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,</div><div class="line">                                                      training_helper,</div><div class="line">                                                      encoder_state,</div><div class="line">                                                      output_layer)</div><div class="line">    </div><div class="line">    training_decoder_output, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,</div><div class="line">                                                                  impute_finished=<span class="keyword">True</span>,</div><div class="line">                                                                  maximum_iterations=max_summary_length)</div><div class="line">    <span class="keyword">return</span> training_decoder_output</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line">DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</div><div class="line">"""</div><div class="line">tests.test_decoding_layer_train(decoding_layer_train)</div></pre></td></tr></table></figure>
<h3 id="Decoding-Inference"><a href="#Decoding-Inference" class="headerlink" title="Decoding - Inference"></a>Decoding - Inference</h3><p>Create inference decoder:</p>
<ul>
<li>Create a <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/GreedyEmbeddingHelper" target="_blank" rel="external"><code>tf.contrib.seq2seq.GreedyEmbeddingHelper</code></a></li>
<li>Create a <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BasicDecoder" target="_blank" rel="external"><code>tf.contrib.seq2seq.BasicDecoder</code></a></li>
<li>Obtain the decoder outputs from <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode" target="_blank" rel="external"><code>tf.contrib.seq2seq.dynamic_decode</code></a></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoding_layer_infer</span><span class="params">(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,</span></span></div><div class="line">                         end_of_sequence_id, max_target_sequence_length,</div><div class="line">                         vocab_size, output_layer, batch_size, keep_prob):</div><div class="line">    <span class="string">"""</span></div><div class="line">    Create a decoding layer for inference</div><div class="line">    :param encoder_state: Encoder state</div><div class="line">    :param dec_cell: Decoder RNN Cell</div><div class="line">    :param dec_embeddings: Decoder embeddings</div><div class="line">    :param start_of_sequence_id: GO ID</div><div class="line">    :param end_of_sequence_id: EOS Id</div><div class="line">    :param max_target_sequence_length: Maximum length of target sequences</div><div class="line">    :param vocab_size: Size of decoder/target vocabulary</div><div class="line">    :param decoding_scope: TenorFlow Variable Scope for decoding</div><div class="line">    :param output_layer: Function to apply the output layer</div><div class="line">    :param batch_size: Batch size</div><div class="line">    :param keep_prob: Dropout keep probability</div><div class="line">    :return: BasicDecoderOutput containing inference logits and sample_id</div><div class="line">    """</div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement Function</span></div><div class="line">    start_tokens = tf.tile(tf.constant([start_of_sequence_id], dtype=tf.int32), [batch_size], name=<span class="string">'start_tokens'</span>)</div><div class="line">    </div><div class="line">    </div><div class="line">    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings,</div><div class="line">                                                                start_tokens,</div><div class="line">                                                                end_of_sequence_id)</div><div class="line"></div><div class="line">    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,</div><div class="line">                                                        inference_helper,</div><div class="line">                                                        encoder_state,</div><div class="line">                                                        output_layer)</div><div class="line">    </div><div class="line">    inference_decoder_output, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,</div><div class="line">                                                                    impute_finished=<span class="keyword">True</span>,</div><div class="line">                                                                    maximum_iterations=max_target_sequence_length)</div><div class="line">    <span class="keyword">return</span> inference_decoder_output</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line">DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</div><div class="line">"""</div><div class="line">tests.test_decoding_layer_infer(decoding_layer_infer)</div></pre></td></tr></table></figure>
<h3 id="Build-the-Decoding-Layer"><a href="#Build-the-Decoding-Layer" class="headerlink" title="Build the Decoding Layer"></a>Build the Decoding Layer</h3><p>Implement <code>decoding_layer()</code> to create a Decoder RNN layer.</p>
<ul>
<li>Embed the target sequences</li>
<li>Construct the decoder LSTM cell (just like you constructed the encoder cell above)</li>
<li>Create an output layer to map the outputs of the decoder to the elements of our vocabulary</li>
<li>Use the your <code>decoding_layer_train(encoder_state, dec_cell, dec_embed_input, target_sequence_length, max_target_sequence_length, output_layer, keep_prob)</code> function to get the training logits.</li>
<li>Use your <code>decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id, max_target_sequence_length, vocab_size, output_layer, batch_size, keep_prob)</code> function to get the inference logits.</li>
</ul>
<p>Note: You’ll need to use <a href="https://www.tensorflow.org/api_docs/python/tf/variable_scope" target="_blank" rel="external">tf.variable_scope</a> to share variables between training and inference.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoding_layer</span><span class="params">(dec_input, encoder_state,</span></span></div><div class="line">                   target_sequence_length, max_target_sequence_length,</div><div class="line">                   rnn_size,</div><div class="line">                   num_layers, target_vocab_to_int, target_vocab_size,</div><div class="line">                   batch_size, keep_prob, decoding_embedding_size):</div><div class="line">    <span class="string">"""</span></div><div class="line">    Create decoding layer</div><div class="line">    :param dec_input: Decoder input</div><div class="line">    :param encoder_state: Encoder state</div><div class="line">    :param target_sequence_length: The lengths of each sequence in the target batch</div><div class="line">    :param max_target_sequence_length: Maximum length of target sequences</div><div class="line">    :param rnn_size: RNN Size</div><div class="line">    :param num_layers: Number of layers</div><div class="line">    :param target_vocab_to_int: Dictionary to go from the target words to an id</div><div class="line">    :param target_vocab_size: Size of target vocabulary</div><div class="line">    :param batch_size: The size of the batch</div><div class="line">    :param keep_prob: Dropout keep probability</div><div class="line">    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)</div><div class="line">    """</div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement Function</span></div><div class="line">    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))</div><div class="line">    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)</div><div class="line"></div><div class="line">    <span class="comment"># 2. Construct the decoder cell</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_cell</span><span class="params">(rnn_size)</span>:</span></div><div class="line">        dec_cell = tf.contrib.rnn.LSTMCell(rnn_size,</div><div class="line">                                           initializer=tf.random_uniform_initializer(<span class="number">-0.1</span>, <span class="number">0.1</span>, seed=<span class="number">2</span>))</div><div class="line">        </div><div class="line">        dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob=keep_prob)</div><div class="line">        </div><div class="line">        <span class="keyword">return</span> dec_cell</div><div class="line"></div><div class="line">    dec_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers)])</div><div class="line">     </div><div class="line">    <span class="comment"># 3. Dense layer to translate the decoder's output at each time </span></div><div class="line">    <span class="comment"># step into a choice from the target vocabulary</span></div><div class="line">    output_layer = Dense(target_vocab_size,</div><div class="line">                         kernel_initializer = tf.truncated_normal_initializer(mean = <span class="number">0.0</span>, stddev=<span class="number">0.1</span>))</div><div class="line">    </div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"decode"</span>):</div><div class="line">        training_logits = decoding_layer_train(encoder_state, dec_cell, dec_embed_input, target_sequence_length, </div><div class="line">                                               max_target_sequence_length, output_layer, keep_prob)</div><div class="line">        </div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"decode"</span>, reuse=<span class="keyword">True</span>):</div><div class="line">        infer_logits = decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, </div><div class="line">                                            target_vocab_to_int[<span class="string">'&lt;GO&gt;'</span>], target_vocab_to_int[<span class="string">'&lt;EOS&gt;'</span>], </div><div class="line">                                            max_target_sequence_length, target_vocab_size, output_layer, batch_size, keep_prob)</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> training_logits, infer_logits</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line">DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</div><div class="line">"""</div><div class="line">tests.test_decoding_layer(decoding_layer)</div></pre></td></tr></table></figure>
<h3 id="Build-the-Neural-Network-1"><a href="#Build-the-Neural-Network-1" class="headerlink" title="Build the Neural Network"></a>Build the Neural Network</h3><p>Apply the functions you implemented above to:</p>
<ul>
<li>Apply embedding to the input data for the encoder.</li>
<li>Encode the input using your <code>encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob,  source_sequence_length, source_vocab_size, encoding_embedding_size)</code>.</li>
<li>Process target data using your <code>process_decoder_input(target_data, target_vocab_to_int, batch_size)</code> function.</li>
<li>Apply embedding to the target data for the decoder.</li>
<li>Decode the encoded input using your <code>decoding_layer(dec_input, enc_state, target_sequence_length, max_target_sentence_length, rnn_size, num_layers, target_vocab_to_int, target_vocab_size, batch_size, keep_prob, dec_embedding_size)</code> function.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">seq2seq_model</span><span class="params">(input_data, target_data, keep_prob, batch_size,</span></span></div><div class="line">                  source_sequence_length, target_sequence_length,</div><div class="line">                  max_target_sentence_length,</div><div class="line">                  source_vocab_size, target_vocab_size,</div><div class="line">                  enc_embedding_size, dec_embedding_size,</div><div class="line">                  rnn_size, num_layers, target_vocab_to_int):</div><div class="line">    <span class="string">"""</span></div><div class="line">    Build the Sequence-to-Sequence part of the neural network</div><div class="line">    :param input_data: Input placeholder</div><div class="line">    :param target_data: Target placeholder</div><div class="line">    :param keep_prob: Dropout keep probability placeholder</div><div class="line">    :param batch_size: Batch Size</div><div class="line">    :param source_sequence_length: Sequence Lengths of source sequences in the batch</div><div class="line">    :param target_sequence_length: Sequence Lengths of target sequences in the batch</div><div class="line">    :param source_vocab_size: Source vocabulary size</div><div class="line">    :param target_vocab_size: Target vocabulary size</div><div class="line">    :param enc_embedding_size: Decoder embedding size</div><div class="line">    :param dec_embedding_size: Encoder embedding size</div><div class="line">    :param rnn_size: RNN Size</div><div class="line">    :param num_layers: Number of layers</div><div class="line">    :param target_vocab_to_int: Dictionary to go from the target words to an id</div><div class="line">    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)</div><div class="line">    """</div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement Function</span></div><div class="line">    enc_ouput, enc_state = encoding_layer(input_data, rnn_size, num_layers, keep_prob,</div><div class="line">                              source_sequence_length, source_vocab_size,</div><div class="line">                              enc_embedding_size)</div><div class="line">    </div><div class="line">    dec_input = process_decoder_input(target_data, target_vocab_to_int, batch_size)</div><div class="line">    </div><div class="line">    training_logits, infer_logits = decoding_layer(dec_input, enc_state,</div><div class="line">                                                   target_sequence_length, max_target_sentence_length,</div><div class="line">                                                   rnn_size,</div><div class="line">                                                   num_layers, target_vocab_to_int, target_vocab_size,</div><div class="line">                                                   batch_size, keep_prob, dec_embedding_size)</div><div class="line">    <span class="keyword">return</span> training_logits, infer_logits</div><div class="line"></div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line">DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</div><div class="line">"""</div><div class="line">tests.test_seq2seq_model(seq2seq_model)</div></pre></td></tr></table></figure>
<h2 id="Neural-Network-Training"><a href="#Neural-Network-Training" class="headerlink" title="Neural Network Training"></a>Neural Network Training</h2><h3 id="Hyperparameters"><a href="#Hyperparameters" class="headerlink" title="Hyperparameters"></a>Hyperparameters</h3><p>Tune the following parameters:</p>
<ul>
<li>Set <code>epochs</code> to the number of epochs.</li>
<li>Set <code>batch_size</code> to the batch size.</li>
<li>Set <code>rnn_size</code> to the size of the RNNs.</li>
<li>Set <code>num_layers</code> to the number of layers.</li>
<li>Set <code>encoding_embedding_size</code> to the size of the embedding for the encoder.</li>
<li>Set <code>decoding_embedding_size</code> to the size of the embedding for the decoder.</li>
<li>Set <code>learning_rate</code> to the learning rate.</li>
<li>Set <code>keep_probability</code> to the Dropout keep probability</li>
<li>Set <code>display_step</code> to state how many steps between each debug output statement</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Number of Epochs</span></div><div class="line">epochs = <span class="number">20</span></div><div class="line"><span class="comment"># Batch Size</span></div><div class="line">batch_size = <span class="number">128</span></div><div class="line"><span class="comment"># RNN Size</span></div><div class="line">rnn_size = <span class="number">80</span></div><div class="line"><span class="comment"># Number of Layers</span></div><div class="line">num_layers = <span class="number">2</span></div><div class="line"><span class="comment"># Embedding Size</span></div><div class="line">encoding_embedding_size = <span class="number">13</span></div><div class="line">decoding_embedding_size = <span class="number">13</span></div><div class="line"><span class="comment"># Learning Rate</span></div><div class="line">learning_rate = <span class="number">0.001</span></div><div class="line"><span class="comment"># Dropout Keep Probability</span></div><div class="line">keep_probability = <span class="number">0.7</span></div><div class="line">display_step = <span class="number">10</span></div></pre></td></tr></table></figure>
<h3 id="Build-the-Graph"><a href="#Build-the-Graph" class="headerlink" title="Build the Graph"></a>Build the Graph</h3><p>Build the graph using the neural network you implemented.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""</span></div><div class="line">DON'T MODIFY ANYTHING IN THIS CELL</div><div class="line">"""</div><div class="line">save_path = <span class="string">'checkpoints/dev'</span></div><div class="line">(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper.load_preprocess()</div><div class="line">max_target_sentence_length = max([len(sentence) <span class="keyword">for</span> sentence <span class="keyword">in</span> source_int_text])</div><div class="line"></div><div class="line">train_graph = tf.Graph()</div><div class="line"><span class="keyword">with</span> train_graph.as_default():</div><div class="line">    input_data, targets, lr, keep_prob, target_sequence_length, max_target_sequence_length, source_sequence_length = model_inputs()</div><div class="line"></div><div class="line">    <span class="comment">#sequence_length = tf.placeholder_with_default(max_target_sentence_length, None, name='sequence_length')</span></div><div class="line">    input_shape = tf.shape(input_data)</div><div class="line"></div><div class="line">    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [<span class="number">-1</span>]),</div><div class="line">                                                   targets,</div><div class="line">                                                   keep_prob,</div><div class="line">                                                   batch_size,</div><div class="line">                                                   source_sequence_length,</div><div class="line">                                                   target_sequence_length,</div><div class="line">                                                   max_target_sequence_length,</div><div class="line">                                                   len(source_vocab_to_int),</div><div class="line">                                                   len(target_vocab_to_int),</div><div class="line">                                                   encoding_embedding_size,</div><div class="line">                                                   decoding_embedding_size,</div><div class="line">                                                   rnn_size,</div><div class="line">                                                   num_layers,</div><div class="line">                                                   target_vocab_to_int)</div><div class="line"></div><div class="line"></div><div class="line">    training_logits = tf.identity(train_logits.rnn_output, name=<span class="string">'logits'</span>)</div><div class="line">    inference_logits = tf.identity(inference_logits.sample_id, name=<span class="string">'predictions'</span>)</div><div class="line"></div><div class="line">    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name=<span class="string">'masks'</span>)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"optimization"</span>):</div><div class="line">        <span class="comment"># Loss function</span></div><div class="line">        cost = tf.contrib.seq2seq.sequence_loss(</div><div class="line">            training_logits,</div><div class="line">            targets,</div><div class="line">            masks)</div><div class="line"></div><div class="line">        <span class="comment"># Optimizer</span></div><div class="line">        optimizer = tf.train.AdamOptimizer(lr)</div><div class="line"></div><div class="line">        <span class="comment"># Gradient Clipping</span></div><div class="line">        gradients = optimizer.compute_gradients(cost)</div><div class="line">        capped_gradients = [(tf.clip_by_value(grad, <span class="number">-1.</span>, <span class="number">1.</span>), var) <span class="keyword">for</span> grad, var <span class="keyword">in</span> gradients <span class="keyword">if</span> grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>]</div><div class="line">        train_op = optimizer.apply_gradients(capped_gradients)</div></pre></td></tr></table></figure>
<p>Batch and pad the source and target sequences</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""</span></div><div class="line">DON'T MODIFY ANYTHING IN THIS CELL</div><div class="line">"""</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_sentence_batch</span><span class="params">(sentence_batch, pad_int)</span>:</span></div><div class="line">    <span class="string">"""Pad sentences with &lt;PAD&gt; so that each sentence of a batch has the same length"""</span></div><div class="line">    max_sentence = max([len(sentence) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentence_batch])</div><div class="line">    <span class="keyword">return</span> [sentence + [pad_int] * (max_sentence - len(sentence)) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentence_batch]</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batches</span><span class="params">(sources, targets, batch_size, source_pad_int, target_pad_int)</span>:</span></div><div class="line">    <span class="string">"""Batch targets, sources, and the lengths of their sentences together"""</span></div><div class="line">    <span class="keyword">for</span> batch_i <span class="keyword">in</span> range(<span class="number">0</span>, len(sources)//batch_size):</div><div class="line">        start_i = batch_i * batch_size</div><div class="line"></div><div class="line">        <span class="comment"># Slice the right amount for the batch</span></div><div class="line">        sources_batch = sources[start_i:start_i + batch_size]</div><div class="line">        targets_batch = targets[start_i:start_i + batch_size]</div><div class="line"></div><div class="line">        <span class="comment"># Pad</span></div><div class="line">        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))</div><div class="line">        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))</div><div class="line"></div><div class="line">        <span class="comment"># Need the lengths for the _lengths parameters</span></div><div class="line">        pad_targets_lengths = []</div><div class="line">        <span class="keyword">for</span> target <span class="keyword">in</span> pad_targets_batch:</div><div class="line">            pad_targets_lengths.append(len(target))</div><div class="line"></div><div class="line">        pad_source_lengths = []</div><div class="line">        <span class="keyword">for</span> source <span class="keyword">in</span> pad_sources_batch:</div><div class="line">            pad_source_lengths.append(len(source))</div><div class="line"></div><div class="line">        <span class="keyword">yield</span> pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths</div></pre></td></tr></table></figure>
<h3 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h3><p>Train the neural network on the preprocessed data. If you have a hard time getting a good loss, check the forms to see if anyone is having the same problem.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""</span></div><div class="line">DON'T MODIFY ANYTHING IN THIS CELL</div><div class="line">"""</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_accuracy</span><span class="params">(target, logits)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Calculate accuracy</div><div class="line">    """</div><div class="line">    max_seq = max(target.shape[<span class="number">1</span>], logits.shape[<span class="number">1</span>])</div><div class="line">    <span class="keyword">if</span> max_seq - target.shape[<span class="number">1</span>]:</div><div class="line">        target = np.pad(</div><div class="line">            target,</div><div class="line">            [(<span class="number">0</span>,<span class="number">0</span>),(<span class="number">0</span>,max_seq - target.shape[<span class="number">1</span>])],</div><div class="line">            <span class="string">'constant'</span>)</div><div class="line">    <span class="keyword">if</span> max_seq - logits.shape[<span class="number">1</span>]:</div><div class="line">        logits = np.pad(</div><div class="line">            logits,</div><div class="line">            [(<span class="number">0</span>,<span class="number">0</span>),(<span class="number">0</span>,max_seq - logits.shape[<span class="number">1</span>])],</div><div class="line">            <span class="string">'constant'</span>)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> np.mean(np.equal(target, logits))</div><div class="line"></div><div class="line"><span class="comment"># Split data to training and validation sets</span></div><div class="line">train_source = source_int_text[batch_size:]</div><div class="line">train_target = target_int_text[batch_size:]</div><div class="line">valid_source = source_int_text[:batch_size]</div><div class="line">valid_target = target_int_text[:batch_size]</div><div class="line">(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,</div><div class="line">                                                                                                             valid_target,</div><div class="line">                                                                                                             batch_size,</div><div class="line">                                                                                                             source_vocab_to_int[<span class="string">'&lt;PAD&gt;'</span>],</div><div class="line">                                                                                                             target_vocab_to_int[<span class="string">'&lt;PAD&gt;'</span>]))                                                                                                  </div><div class="line"><span class="keyword">with</span> tf.Session(graph=train_graph) <span class="keyword">as</span> sess:</div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line"></div><div class="line">    <span class="keyword">for</span> epoch_i <span class="keyword">in</span> range(epochs):</div><div class="line">        <span class="keyword">for</span> batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) <span class="keyword">in</span> enumerate(</div><div class="line">                get_batches(train_source, train_target, batch_size,</div><div class="line">                            source_vocab_to_int[<span class="string">'&lt;PAD&gt;'</span>],</div><div class="line">                            target_vocab_to_int[<span class="string">'&lt;PAD&gt;'</span>])):</div><div class="line"></div><div class="line">            _, loss = sess.run(</div><div class="line">                [train_op, cost],</div><div class="line">                &#123;input_data: source_batch,</div><div class="line">                 targets: target_batch,</div><div class="line">                 lr: learning_rate,</div><div class="line">                 target_sequence_length: targets_lengths,</div><div class="line">                 source_sequence_length: sources_lengths,</div><div class="line">                 keep_prob: keep_probability&#125;)</div><div class="line"></div><div class="line"></div><div class="line">            <span class="keyword">if</span> batch_i % display_step == <span class="number">0</span> <span class="keyword">and</span> batch_i &gt; <span class="number">0</span>:</div><div class="line"></div><div class="line"></div><div class="line">                batch_train_logits = sess.run(</div><div class="line">                    inference_logits,</div><div class="line">                    &#123;input_data: source_batch,</div><div class="line">                     source_sequence_length: sources_lengths,</div><div class="line">                     target_sequence_length: targets_lengths,</div><div class="line">                     keep_prob: <span class="number">1.0</span>&#125;)</div><div class="line"></div><div class="line"></div><div class="line">                batch_valid_logits = sess.run(</div><div class="line">                    inference_logits,</div><div class="line">                    &#123;input_data: valid_sources_batch,</div><div class="line">                     source_sequence_length: valid_sources_lengths,</div><div class="line">                     target_sequence_length: valid_targets_lengths,</div><div class="line">                     keep_prob: <span class="number">1.0</span>&#125;)</div><div class="line"></div><div class="line">                train_acc = get_accuracy(target_batch, batch_train_logits)</div><div class="line"></div><div class="line">                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)</div><div class="line"></div><div class="line">                print(<span class="string">'Epoch &#123;:&gt;3&#125; Batch &#123;:&gt;4&#125;/&#123;&#125; - Train Accuracy: &#123;:&gt;6.4f&#125;, Validation Accuracy: &#123;:&gt;6.4f&#125;, Loss: &#123;:&gt;6.4f&#125;'</span></div><div class="line">                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))</div><div class="line"></div><div class="line">    <span class="comment"># Save Model</span></div><div class="line">    saver = tf.train.Saver()</div><div class="line">    saver.save(sess, save_path)</div><div class="line">    print(<span class="string">'Model Trained and Saved'</span>)</div></pre></td></tr></table></figure>
<h3 id="Save-Parameters"><a href="#Save-Parameters" class="headerlink" title="Save Parameters"></a>Save Parameters</h3><p>Save the <code>batch_size</code> and <code>save_path</code> parameters for inference.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""</span></div><div class="line">DON'T MODIFY ANYTHING IN THIS CELL</div><div class="line">"""</div><div class="line"><span class="comment"># Save parameters for checkpoint</span></div><div class="line">helper.save_params(save_path)</div></pre></td></tr></table></figure>
<h1 id="Checkpoint"><a href="#Checkpoint" class="headerlink" title="Checkpoint"></a>Checkpoint</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""</span></div><div class="line">DON'T MODIFY ANYTHING IN THIS CELL</div><div class="line">"""</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> helper</div><div class="line"><span class="keyword">import</span> problem_unittests <span class="keyword">as</span> tests</div><div class="line"></div><div class="line">_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = helper.load_preprocess()</div><div class="line">load_path = helper.load_params()</div></pre></td></tr></table></figure>
<h2 id="Sentence-to-Sequence"><a href="#Sentence-to-Sequence" class="headerlink" title="Sentence to Sequence"></a>Sentence to Sequence</h2><p>To feed a sentence into the model for translation, you first need to preprocess it.  Implement the function <code>sentence_to_seq()</code> to preprocess new sentences.</p>
<ul>
<li>Convert the sentence to lowercase</li>
<li>Convert words into ids using <code>vocab_to_int</code><ul>
<li>Convert words not in the vocabulary, to the <code>&lt;UNK&gt;</code> word id.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentence_to_seq</span><span class="params">(sentence, vocab_to_int)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Convert a sentence to a sequence of ids</div><div class="line">    :param sentence: String</div><div class="line">    :param vocab_to_int: Dictionary to go from the words to an id</div><div class="line">    :return: List of word ids</div><div class="line">    """</div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement Function</span></div><div class="line">    vocab = sentence.lower().split()</div><div class="line">    word_ids = []</div><div class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> vocab:</div><div class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocab_to_int:</div><div class="line">            word_ids.append(vocab_to_int[word])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            word_ids.append(vocab_to_int[<span class="string">'&lt;UNK&gt;'</span>])</div><div class="line">    <span class="keyword">return</span> word_ids</div><div class="line"></div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line">DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</div><div class="line">"""</div><div class="line">tests.test_sentence_to_seq(sentence_to_seq)</div></pre></td></tr></table></figure>
<h2 id="Translate"><a href="#Translate" class="headerlink" title="Translate"></a>Translate</h2><p>This will translate <code>translate_sentence</code> from English to French.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">translate_sentence = <span class="string">'he saw a old yellow truck .'</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line">DON'T MODIFY ANYTHING IN THIS CELL</div><div class="line">"""</div><div class="line">translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)</div><div class="line"></div><div class="line">loaded_graph = tf.Graph()</div><div class="line"><span class="keyword">with</span> tf.Session(graph=loaded_graph) <span class="keyword">as</span> sess:</div><div class="line">    <span class="comment"># Load saved model</span></div><div class="line">    loader = tf.train.import_meta_graph(load_path + <span class="string">'.meta'</span>)</div><div class="line">    loader.restore(sess, load_path)</div><div class="line"></div><div class="line">    input_data = loaded_graph.get_tensor_by_name(<span class="string">'input:0'</span>)</div><div class="line">    logits = loaded_graph.get_tensor_by_name(<span class="string">'predictions:0'</span>)</div><div class="line">    target_sequence_length = loaded_graph.get_tensor_by_name(<span class="string">'target_sequence_length:0'</span>)</div><div class="line">    source_sequence_length = loaded_graph.get_tensor_by_name(<span class="string">'source_sequence_length:0'</span>)</div><div class="line">    keep_prob = loaded_graph.get_tensor_by_name(<span class="string">'keep_prob:0'</span>)</div><div class="line"></div><div class="line">    translate_logits = sess.run(logits, &#123;input_data: [translate_sentence]*batch_size,</div><div class="line">                                         target_sequence_length: [len(translate_sentence)*<span class="number">2</span>]*batch_size,</div><div class="line">                                         source_sequence_length: [len(translate_sentence)]*batch_size,</div><div class="line">                                         keep_prob: <span class="number">1.0</span>&#125;)[<span class="number">0</span>]</div><div class="line"></div><div class="line">print(<span class="string">'Input'</span>)</div><div class="line">print(<span class="string">'  Word Ids:      &#123;&#125;'</span>.format([i <span class="keyword">for</span> i <span class="keyword">in</span> translate_sentence]))</div><div class="line">print(<span class="string">'  English Words: &#123;&#125;'</span>.format([source_int_to_vocab[i] <span class="keyword">for</span> i <span class="keyword">in</span> translate_sentence]))</div><div class="line"></div><div class="line">print(<span class="string">'\nPrediction'</span>)</div><div class="line">print(<span class="string">'  Word Ids:      &#123;&#125;'</span>.format([i <span class="keyword">for</span> i <span class="keyword">in</span> translate_logits]))</div><div class="line">print(<span class="string">'  French Words: &#123;&#125;'</span>.format(<span class="string">" "</span>.join([target_int_to_vocab[i] <span class="keyword">for</span> i <span class="keyword">in</span> translate_logits])))</div></pre></td></tr></table></figure>
<h2 id="Imperfect-Translation"><a href="#Imperfect-Translation" class="headerlink" title="Imperfect Translation"></a>Imperfect Translation</h2><p>You might notice that some sentences translate better than others.  Since the dataset you’re using only has a vocabulary of 227 English words of the thousands that you use, you’re only going to see good results using these words.  For this project, you don’t need a perfect translation. However, if you want to create a better translation model, you’ll need better data.</p>
<p>You can train on the <a href="http://www.statmt.org/wmt10/training-giga-fren.tar" target="_blank" rel="external">WMT10 French-English corpus</a>.  This dataset has more vocabulary and richer in topics discussed.  However, this will take you days to train, so make sure you’ve a GPU and the neural network is performing well on dataset we provided.  Just make sure you play with the WMT10 corpus after you’ve submitted this project.</p>
<h2 id="Submitting-This-Project"><a href="#Submitting-This-Project" class="headerlink" title="Submitting This Project"></a>Submitting This Project</h2><p>When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as “dlnd_language_translation.ipynb” and save it as a HTML file under “File” -&gt; “Download as”. Include the “helper.py” and “problem_unittests.py” files in your submission.</p>

    </div>

    

    
        <div class="post-tags">
            <i class="fa fa-tags" aria-hidden="true"></i>
            <a href="/tags/DeepLearning/">#DeepLearning</a>
        </div>
    

    <!-- Comments -->
    

</div>
        </section>

    </div>
</div>

</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    Supported by Hexo
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2017/07/05/language-translation/">language-translation</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2017/06/22/Autoencoder/">Autoencoder</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2015/12/31/first-post/">DigitalOcean&#39;s First Post</a>
            </li>
            
        </ul>
    </div>



            
<div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 footer-categories">
    <h2>Categories</h2>
    <ul>
        
        <li>
            <a class="footer-post" href="/categories/Hexo/">Hexo</a>
        </li>
        
    </ul>
</div>

        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    
                    <li class="list-inline-item">
                        <a href="https://twitter.com/?lang=en">
                            <span class="footer-icon-container">
                                <i class="fa fa-twitter"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.facebook.com/">
                            <span class="footer-icon-container">
                                <i class="fa fa-facebook"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.instagram.com">
                            <span class="footer-icon-container">
                                <i class="fa fa-instagram"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    
                    
                    
                    
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @Milkrong. All right reserved 
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Custom JavaScript -->
<script src="/js/main.js"></script>

<!-- Disqus Comments -->



</body>

</html>